"""LLM æœåŠ¡æ ¸å¿ƒå®ç°ã€‚

ä½¿ç”¨ LiteLLM ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒ OpenAIã€Anthropicã€Gemini ä»¥åŠå…¼å®¹ OpenAI æ ¼å¼çš„ç¬¬ä¸‰æ–¹æœåŠ¡ã€‚
"""

import os
import json
import logging
from dataclasses import asdict, is_dataclass
from typing import Any, Dict, List, Optional
from pathlib import Path
from dotenv import load_dotenv

import litellm

try:
    from litellm.exceptions import LiteLLMException
except Exception:  # noqa: BLE001
    LiteLLMException = Exception

from core.llm.cache import generate_cache_key
from core.storage.interfaces import CacheStorage
from config.settings import settings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = logging.getLogger("core.llm.service")


class LLMService:
    """LLM æœåŠ¡ç±»ï¼Œå°è£… LiteLLM è°ƒç”¨ã€‚"""

    def __init__(self, cache_storage: CacheStorage | None = None):
        self.config = self._load_config()
        self.cache = cache_storage
        self._setup_litellm()

    @staticmethod
    def _mask_api_key(api_key: Optional[str]) -> str:
        """ä»…æš´éœ² API Key å‰ 8 ä½ï¼Œé¿å…æ—¥å¿—æ³„éœ²ã€‚"""
        if not api_key:
            return ""
        prefix = api_key[:8]
        return f"{prefix}***"

    @staticmethod
    def _response_to_dict(response: Any) -> Dict[str, Any]:
        """ç»Ÿä¸€è½¬æˆåŸç”Ÿå­—å…¸ï¼Œä¾¿äºåç»­ç¼“å­˜ä¸åºåˆ—åŒ–ã€‚"""
        if isinstance(response, dict):
            return response
        if isinstance(response, str):
            try:
                parsed = json.loads(response)
            except json.JSONDecodeError as exc:
                raise ValueError("å“åº”å­—ç¬¦ä¸²ä¸æ˜¯åˆæ³• JSON æ•°æ®") from exc
            if isinstance(parsed, dict):
                return parsed
            return {"data": parsed}
        if hasattr(response, "model_dump"):
            return response.model_dump()
        if hasattr(response, "dict"):
            return response.dict()
        if hasattr(response, "json"):
            try:
                raw_json = response.json()
                parsed = json.loads(raw_json) if isinstance(raw_json, str) else raw_json
                if isinstance(parsed, dict):
                    return parsed
                return {"data": parsed}
            except (TypeError, json.JSONDecodeError) as exc:
                raise ValueError("å“åº” json() ç»“æœä¸æ˜¯åˆæ³• JSON æ•°æ®") from exc
            except Exception as exc:  # noqa: BLE001
                raise ValueError("å“åº”å¯¹è±¡çš„ json() æ–¹æ³•æ‰§è¡Œå¤±è´¥") from exc
        # æœ€åå°è¯•ç›´æ¥åºåˆ—åŒ–ï¼Œå†è§£æå›å­—å…¸
        def _default_serializer(obj: Any) -> Any:
            if is_dataclass(obj):
                return asdict(obj)
            if hasattr(obj, "__dict__"):
                return obj.__dict__
            raise TypeError(f"å¯¹è±¡ {type(obj).__name__} æ— æ³•è¢«åºåˆ—åŒ–")

        try:
            serialized = json.dumps(response, default=_default_serializer)
            parsed = json.loads(serialized)
        except (TypeError, ValueError) as exc:
            raise ValueError("æ— æ³•å°†å“åº”åºåˆ—åŒ–ä¸º JSON") from exc
        if isinstance(parsed, dict):
            return parsed
        return {"data": parsed}

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡æ˜¯ settings.jsonã€‚"""
        settings_path = Path("config/settings.json")
        file_config = {}
        
        if settings_path.exists():
            try:
                with open(settings_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    file_config = data.get("llm", {})
            except Exception as e:
                logger.error(f"åŠ è½½ settings.json å¤±è´¥: {e}")

        # ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§æ›´é«˜
        config = {
            "provider": os.getenv("LLM_PROVIDER", file_config.get("provider", "openai")),
            "model": os.getenv("LLM_MODEL", file_config.get("model", "gpt-4")),
            "api_key": os.getenv("LLM_API_KEY", file_config.get("api_key", "")),
            "base_url": os.getenv("LLM_BASE_URL", file_config.get("base_url", "")),
            "api_version": os.getenv("LLM_API_VERSION", file_config.get("api_version", "")),
        }
        
        return config

    def _setup_litellm(self):
        """é…ç½® LiteLLMã€‚"""
        # è®¾ç½® API Key
        if self.config["api_key"]:
            # LiteLLM ä¼šè‡ªåŠ¨æŸ¥æ‰¾ç¯å¢ƒå˜é‡ï¼Œä½†è¿™é‡Œæ˜¾å¼è®¾ç½®æ›´å®‰å…¨
            # æ³¨æ„ï¼šä¸åŒ provider éœ€è¦ä¸åŒçš„ç¯å¢ƒå˜é‡åï¼Œä½† LiteLLM æ”¯æŒé€šè¿‡å‚æ•°ä¼ é€’ api_key
            pass
        
        # é…ç½®æ—¥å¿—
        litellm.set_verbose = False  # è®¾ç½®ä¸º True å¯å¼€å¯è¯¦ç»†è°ƒè¯•æ—¥å¿—
        
        # è‡ªåŠ¨ä¸¢å¼ƒæ¨¡å‹ä¸æ”¯æŒçš„å‚æ•°ï¼Œé¿å… GPT-5 ç­‰æ¨¡å‹æŠ¥é”™
        litellm.drop_params = True
        logger.info("âœ… LiteLLM é…ç½®ï¼šè‡ªåŠ¨ä¸¢å¼ƒä¸æ”¯æŒçš„å‚æ•° (drop_params=True)")

    def _resolve_request_url(self, provider: str, params: Dict[str, Any]) -> Optional[str]:
        """å°è¯•æ¨æ–­çœŸå®çš„ HTTP è¯·æ±‚ URLã€‚"""
        endpoint = self._guess_endpoint(provider, params)
        provider_lower = provider.lower()

        api_base = params.get("api_base") or self.config.get("base_url")
        if api_base:
            return self._compose_url(api_base, endpoint, provider_lower, params)

        default_bases = {
            "openai": "https://api.openai.com/v1",
            "anthropic": "https://api.anthropic.com/v1",
            "gemini": "https://generativelanguage.googleapis.com/v1beta",
        }
        base_candidate = default_bases.get(provider_lower)
        if base_candidate:
            return self._compose_url(base_candidate, endpoint, provider_lower, params)
        return None

    @staticmethod
    def _compose_url(
        base_url: str,
        endpoint: str,
        provider_lower: str,
        params: Dict[str, Any],
    ) -> str:
        normalized = base_url.rstrip("/")
        query = ""
        if provider_lower.startswith("azure") and "?" not in normalized:
            api_version = params.get("api_version")
            if api_version:
                query = f"?api-version={api_version}"
        return f"{normalized}/{endpoint}{query}"

    @staticmethod
    def _guess_endpoint(provider: str, params: Dict[str, Any]) -> str:
        provider_lower = provider.lower()
        if "anthropic" in provider_lower:
            return "messages"
        if provider_lower.startswith("azure"):
            return "chat/completions"
        if "gemini" in provider_lower or provider_lower == "google":
            model_name = params.get("model")
            if model_name:
                return f"models/{model_name}:generateContent"
            return "models:generateContent"
        if "ollama" in provider_lower:
            return "api/chat"
        return "chat/completions"

    def _log_http_debug_response(self, resp: Any, request_url: Optional[str]) -> None:
        """è®°å½• HTTP å“åº”çš„è°ƒè¯•ä¿¡æ¯ï¼Œå¸®åŠ©å®šä½è§£æå¤±è´¥é—®é¢˜ã€‚"""
        status_code = getattr(resp, "status_code", None)
        headers = getattr(resp, "headers", None)
        content_type: Optional[str] = None
        if headers and hasattr(headers, "get"):
            content_type = headers.get("Content-Type") or headers.get("content-type")

        body_text = ""
        if hasattr(resp, "text"):
            try:
                body_text = resp.text or ""
            except Exception:  # noqa: BLE001
                body_text = ""
        elif hasattr(resp, "content"):
            content = getattr(resp, "content")
            if isinstance(content, bytes):
                body_text = content.decode("utf-8", "ignore")
            else:
                body_text = str(content)
        else:
            body_text = str(resp)

        preview = body_text[:500]
        logger.error(
            "LLM HTTP å“åº”è°ƒè¯•: url=%s, status=%s, content_type=%s, body_preview=%s",
            request_url or "æœªçŸ¥",
            status_code,
            content_type,
            preview,
        )

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        use_cache: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‘é€èŠå¤©è¯·æ±‚åˆ° LLMã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œä¾‹å¦‚ [{"role": "user", "content": "hello"}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼‰ï¼Œå¯¹è¯åœºæ™¯å»ºè®®è®¾ä¸º False
            **kwargs: å…¶ä»– LiteLLM æ”¯æŒçš„å‚æ•°

        Returns:
            LiteLLM çš„å“åº”å¯¹è±¡ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        """
        provider = self.config.get("provider", "openai")
        model = self.config.get("model", "gpt-4")
        params: Dict[str, Any] | None = None

        try:
            # å¯¹äº custom providerï¼ˆOpenAI å…¼å®¹çš„ç¬¬ä¸‰æ–¹ APIï¼‰ï¼Œè½¬æ¢ä¸º openai
            # è¿™æ · LiteLLM ä¼šä½¿ç”¨ OpenAI çš„åè®®æ ¼å¼ + è‡ªå®šä¹‰ api_base
            if provider == "custom":
                provider = "openai"
                logger.info("ğŸ“ æ£€æµ‹åˆ° custom providerï¼Œè½¬æ¢ä¸º openai åè®®æ ¼å¼")

            # æ„å»ºå®Œæ•´çš„æ¨¡å‹åç§°
            # å¦‚æœæ˜¯ openai å…¼å®¹çš„ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œé€šå¸¸ä¸éœ€è¦åŠ  provider å‰ç¼€ï¼Œæˆ–è€…ç›´æ¥ç”¨ model å
            # LiteLLM çº¦å®šï¼šå¯¹äº openai å…¼å®¹æ¥å£ï¼Œå¦‚æœ provider æ˜¯ openaiï¼Œå¯ä»¥ç›´æ¥ç”¨ model å
            # å¦‚æœæ˜¯ anthropic/gemini ç­‰ï¼Œlitellm é€šå¸¸éœ€è¦å‰ç¼€ï¼Œå¦‚ "anthropic/claude-3"
            # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€å•çš„å¤„ç†ï¼šå¦‚æœ provider ä¸æ˜¯ openaiï¼Œä¸” model ä¸åŒ…å« /ï¼Œåˆ™åŠ ä¸Šå‰ç¼€

            full_model_name = model
            if provider == "openai":
                normalized_model = model.split("/", 1)[-1]
                full_model_name = f"openai/{normalized_model}"
            elif "/" not in model:
                full_model_name = f"{provider}/{model}"
            
            # å‡†å¤‡å‚æ•°
            params = {
                "model": full_model_name,
                "messages": messages,
                "temperature": temperature,
                "api_key": self.config["api_key"],
            }

            # GPT-5 ç³»åˆ—æ¨¡å‹åªæ”¯æŒ temperature=1ï¼Œå¿…è¦æ—¶è‡ªåŠ¨çº æ­£
            if "gpt-5" in full_model_name.lower() and temperature != 1.0:
                logger.warning(
                    "âš ï¸  GPT-5 æ¨¡å‹åªæ”¯æŒ temperature=1ï¼Œå·²ä» %.1f è°ƒæ•´ä¸º 1.0",
                    temperature,
                )
                params["temperature"] = 1.0

            # å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šçš„ providerï¼Œé˜²æ­¢ LiteLLM æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨åˆ‡æ¢
            # ä¾‹å¦‚ï¼šæ¨¡å‹åç§°åŒ…å« "claude" æ—¶ï¼ŒLiteLLM ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ° anthropic provider
            # ä½†å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº† openai providerï¼ˆOpenAI å…¼å®¹ APIï¼‰ï¼Œåˆ™åº”è¯¥å°Šé‡ç”¨æˆ·é€‰æ‹©
            if provider == "openai":
                params["custom_llm_provider"] = "openai"

            if max_tokens:
                params["max_tokens"] = max_tokens

            # å¦‚æœæœ‰ base_url (ç”¨äº DeepSeek, Moonshot, Local ç­‰)
            if self.config["base_url"]:
                api_base = self.config["base_url"].rstrip("/")
                if provider == "openai" and not api_base.endswith("/v1"):
                    api_base = f"{api_base}/v1"
                params["api_base"] = api_base
            
            # å¦‚æœæœ‰ api_version
            if self.config["api_version"]:
                params["api_version"] = self.config["api_version"]

            # åˆå¹¶å…¶ä»–å‚æ•°
            params.update(kwargs)

            # æ·»åŠ æµè§ˆå™¨è¯·æ±‚å¤´ä»¥ç»•è¿‡ä¸­è½¬ç«™ block æ£€æµ‹
            # è¿™äº›è¯·æ±‚å¤´æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è®¿é—®ï¼Œé¿å…è¢«åçˆ¬è™«æœºåˆ¶æ‹¦æˆª
            extra_headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                "Accept": "application/json",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            }

            # å¦‚æœæœ‰ base_urlï¼Œæ·»åŠ  Referer å’Œ Origin
            if self.config["base_url"]:
                base_domain = self.config["base_url"].rstrip("/")
                extra_headers["Referer"] = f"{base_domain}/"
                extra_headers["Origin"] = base_domain

            # åˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ˆå¦‚æœæœ‰ï¼‰
            if "extra_headers" in kwargs:
                extra_headers.update(kwargs["extra_headers"])

            params["extra_headers"] = extra_headers

            logger.info(
                "ğŸ“‹ æ·»åŠ æµè§ˆå™¨è¯·æ±‚å¤´: User-Agent=%s, Referer=%s",
                extra_headers.get("User-Agent", "æ— ")[:50],
                extra_headers.get("Referer", "æ— "),
            )

            request_url = self._resolve_request_url(provider, params)

            cache_key = None
            if use_cache and settings.llm_cache_enabled and self.cache:
                cache_key = generate_cache_key(messages, self.config["model"], temperature)
                cached = await self.cache.get(cache_key)
                if cached:
                    logger.info("âœ… LLM ç¼“å­˜å‘½ä¸­: %s", cache_key[:16])
                    return json.loads(cached)

            safe_params = params.copy()
            safe_params["api_key"] = self._mask_api_key(safe_params.get("api_key"))
            logger.info(
                "å‘é€ LLM è¯·æ±‚: url=%s, params=%s",
                request_url or "æœªæ¨æ–­",
                safe_params,
            )

            # è°ƒç”¨ LiteLLM (å¼‚æ­¥)
            raw_response = await litellm.acompletion(**params)
            response_payload: Any | None = None
            if hasattr(raw_response, "json") and callable(getattr(raw_response, "json", None)):
                try:
                    response_payload = raw_response.json()
                except json.JSONDecodeError:
                    self._log_http_debug_response(raw_response, request_url)
                    raise
            if response_payload is not None:
                response = self._response_to_dict(response_payload)
            else:
                response = self._response_to_dict(raw_response)

            if not isinstance(response, dict):
                preview = str(response)[:500]
                logger.error(
                    "LLM å“åº”è§£æå¤±è´¥: url=%s, type=%s, preview=%s",
                    request_url or "æœªæ¨æ–­",
                    type(response).__name__,
                    preview,
                )
                raise ValueError("LLM å“åº”æ ¼å¼é”™è¯¯")

            choices = response.get("choices")
            if not isinstance(choices, list) or not choices:
                preview = json.dumps(response, ensure_ascii=False)[:500]
                logger.error(
                    "LLM å“åº”ç¼ºå°‘ choices: url=%s, keys=%s, preview=%s",
                    request_url or "æœªæ¨æ–­",
                    list(response.keys()),
                    preview,
                )
                raise ValueError("LLM å“åº”ç¼ºå°‘ choices")

            first_choice = choices[0]
            if not isinstance(first_choice, dict):
                preview = str(first_choice)[:300]
                logger.error(
                    "LLM choices[0] ç±»å‹å¼‚å¸¸: url=%s, type=%s, preview=%s",
                    request_url or "æœªæ¨æ–­",
                    type(first_choice).__name__,
                    preview,
                )
                raise ValueError("LLM choices[0] ç±»å‹å¼‚å¸¸")

            message = first_choice.get("message")
            if not isinstance(message, dict) or "content" not in message:
                preview = json.dumps(first_choice, ensure_ascii=False)[:500]
                logger.error(
                    "LLM å“åº” message æ— æ•ˆ: url=%s, preview=%s",
                    request_url or "æœªæ¨æ–­",
                    preview,
                )
                raise ValueError("LLM å“åº”ç¼ºå°‘æœ‰æ•ˆçš„ message.content")

            choice_count = len(choices)
            logger.info(
                "LLM å“åº”ç»“æ„: type=%s, keys=%s, choices=%s, usage=%s",
                type(raw_response).__name__,
                list(response.keys()),
                choice_count,
                response.get("usage"),
            )

            if use_cache and settings.llm_cache_enabled and self.cache and cache_key:
                try:
                    await self.cache.set(cache_key, json.dumps(response), ttl=settings.llm_cache_ttl)
                except Exception as cache_exc:  # noqa: BLE001
                    logger.warning("å†™å…¥ç¼“å­˜å¤±è´¥: %s", cache_exc)

            return response

        except LiteLLMException as api_error:
            safe_params = {}
            if params:
                safe_params = params.copy()
                safe_params["api_key"] = self._mask_api_key(safe_params.get("api_key"))
            logger.error(
                "LLM API é”™è¯¯: provider=%s, model=%s, params=%s, é”™è¯¯=%s",
                provider,
                model,
                safe_params,
                api_error,
                exc_info=True,
            )
            raise
        except Exception as e:  # noqa: BLE001
            logger.exception("LLM è¯·æ±‚å¤±è´¥: %s", e)
            raise

# ä¸å†å¯¼å‡ºæ¨¡å—çº§å•ä¾‹å®ä¾‹ï¼Œå®ä¾‹ç”±ä¾èµ–æ³¨å…¥å·¥å‚ç®¡ç†
